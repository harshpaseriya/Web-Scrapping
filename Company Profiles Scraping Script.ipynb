{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import csv \n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36','Accept-Encoding': 'gzip, deflate, br','Accept-Language': 'en-US,en;q=0.9,hi;q=0.8'}\n",
    "\n",
    "URL = 'https://www.dnb.com/business-directory/company-information.information-technology-services.us.html?page=1'\n",
    "\n",
    "fws = open('States Links.csv', 'w', newline = '')\n",
    "\n",
    "link_part = 'https://www.dnb.com'\n",
    "\n",
    "writer = csv.writer(fws)\n",
    "res = requests.get(URL, headers= headers )\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "container_s = soup.find_all('div', attrs = {'class', 'col-md-6 col-xs-6 data'})\n",
    "\n",
    "for i in container_s:\n",
    "    link_state = i.find('a', href = True)\n",
    "    link_state = link_state['href']\n",
    "    link_state = link_state.replace(link_state[-1], '')\n",
    "    link_state = link_part + link_state\n",
    "    fws.write(link_state)\n",
    "    fws.write('\\n')\n",
    "fws.close()\n",
    "\n",
    "frs = open('States Links.csv' , 'r')\n",
    "csv_reader_s = csv.reader(frs)\n",
    "\n",
    "\n",
    "for j in csv_reader_s:\n",
    "    link_s = str(*j)\n",
    "    URL = link_s\n",
    "\n",
    "    f = open('Company Profile Links.csv', 'w', newline = '')\n",
    "\n",
    "    link_part = 'https://www.dnb.com'\n",
    "\n",
    "    writer = csv.writer(f)\n",
    "    for page in range(1,21):\n",
    "        res = requests.get(URL + str(page), headers= headers )\n",
    "        soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "        container = soup.find_all('div', attrs = {'class', 'col-md-12 data'})\n",
    "\n",
    "        for i in container:\n",
    "            link = i.find('div', attrs = {'class' , 'col-md-6'})\n",
    "            link = link.find('a', href = True)\n",
    "            link = link_part + link ['href']\n",
    "            f.write(link)\n",
    "            f.write('\\n')\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    fw = open('Companies profiles Details.csv', 'w', newline = '')\n",
    "    writer = csv.writer(fw)\n",
    "    writer.writerow(['Company','Street Address','Regional Address','Website','Type','Phone','Revenue','Employees This Site', 'Employees Total', 'Founded', 'Board Members'])\n",
    "\n",
    "    fr = open('Company Profile Links.csv' , 'r')\n",
    "    csv_reader = csv.reader(fr)\n",
    "    URL = ''\n",
    "\n",
    "    for line in csv_reader:\n",
    "        URL  = str(*line)\n",
    "        print(URL)    \n",
    "\n",
    "        try:\n",
    "            Company_Name = ''\n",
    "            Street_Address = ''\n",
    "            Regional_Address= ''\n",
    "            Phone = ''\n",
    "            revenue_total = ''\n",
    "            employees_num_site = ''\n",
    "            employees_num_tot = ''\n",
    "            incorporated = ''\n",
    "            Board_info = ''\n",
    "\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            res = requests.get(URL , headers= headers )\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            container = soup.find_all('div', attrs = {'class', 'meta'})\n",
    "            Company_Name = container[0].find('h1')\n",
    "            Company_Name = Company_Name.text.strip()\n",
    "            print(Company_Name)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "            Street_Address =  soup.find('div', attrs = {'class', 'street_address_1'})\n",
    "            Street_Address = Street_Address.text.strip()\n",
    "            print(Street_Address)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            Regional_Address =  soup.find('div', attrs = {'class', 'company_regional_address'})\n",
    "            Regional_city = Regional_Address.find('span', attrs = {'class' , 'company_city'})\n",
    "            Regional_region = Regional_Address.find('span', attrs = {'class' , 'company_region'})\n",
    "            Regional_postal = Regional_Address.find('span', attrs = {'class' , 'company_postal'})\n",
    "            Regional_country = Regional_Address.find('span', attrs = {'class' , 'company_country'})\n",
    "            Regional_postal = Regional_postal.text.strip()\n",
    "            Regional_postal = str(Regional_postal)\n",
    "            Final_Address = Regional_city.text.strip() + ' ' + Regional_region.text.strip() + ' ' + Regional_postal+ ' ' + Regional_country.text.strip()\n",
    "            print(Final_Address)\n",
    "\n",
    "        except: pass\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            Web_link = soup.find('div', attrs = {'class', 'web'})\n",
    "            Web_link = Web_link.text.strip()\n",
    "            print(Web_link)\n",
    "\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            Type = soup.find('div', attrs = {'class', 'type-role'})\n",
    "            Type = Type.text.strip()\n",
    "            Type = \" \".join(Type.split())\n",
    "            Type = Type.replace('Company Type:', '')\n",
    "            Type = Type.strip()\n",
    "            print(Type)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            Phone = soup.find('div', attrs = {'class', 'phone'})\n",
    "            Phone = Phone.text.strip()\n",
    "            print(Phone)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            revenue_total = soup.find('div' , attrs = {'class' , 'rev_title_number'})\n",
    "            revenue_total = revenue_total.text.strip()\n",
    "            print(revenue_total)\n",
    "        except:pass  \n",
    "\n",
    "        try:\n",
    "\n",
    "            employees_span = soup.find('div' , attrs = {'class' , 'company_info_body module_body'})\n",
    "            employees_site = employees_span.find('li' , attrs = {'class', 'empCon'})\n",
    "            employees_site = employees_site.find('span' , attrs = {'class' , 'value'})\n",
    "            employees_num_tot = employees_site.text.strip()\n",
    "            print(employees_num_tot)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            employees_site = employees_span.find('li' , attrs = {'class', 'emp'})\n",
    "            employees_site = employees_site.find('span' , attrs = {'class' , 'value'})\n",
    "            employees_num_site = employees_site.text.strip()\n",
    "            print(employees_num_site)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            incorporated = employees_span.find('li' , attrs = {'class', 'founded'})\n",
    "            incorporated = incorporated.find('span' , attrs = {'class' , 'value'})\n",
    "            incorporated = incorporated.text.strip()\n",
    "            print(incorporated)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            Board_info = soup.find_all('div' , attrs = {'class' , 'module_body contact-body'})\n",
    "            Board_info = Board_info[0].text.strip()\n",
    "            Board_info = \" \".join(Board_info.split())\n",
    "            print(Board_info)\n",
    "\n",
    "        except:pass\n",
    "\n",
    "        writer.writerow([Company_Name,Street_Address,Final_Address,Web_link,Type,Phone,revenue_total,employees_num_site,employees_num_tot,incorporated,Board_info])\n",
    "fw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
